x-common: &common-config
  restart: always
  logging:
    driver: "json-file"
    options:
      max-size: "100m"
      max-file: "5"
  runtime: nvidia
  environment:
    - NVIDIA_VISIBLE_DEVICES=all
    - TOKEN=<YOUR_TOKEN>
    - BSV_CHAIN=${BSV_CHAIN:-main}
    - NODE_IDENTITY_KEY=${NODE_IDENTITY_KEY}
    - MANDALA_NODE_CALLBACK_URL=${MANDALA_NODE_CALLBACK_URL}
    - RECEIPT_BATCH_SIZE=${RECEIPT_BATCH_SIZE:-100}
    - RECEIPT_BATCH_INTERVAL=${RECEIPT_BATCH_INTERVAL:-300}

services:
  vllm-proxy:
    <<: *common-config
    image: 0xii/vllm-proxy:0.1.1
    container_name: vllm-proxy
    privileged: true
    volumes:
      - /var/run/dstack.sock:/var/run/dstack.sock
    ports:
      - "8000:8000"

  vllm:
    <<: *common-config
    image: vllm/vllm-openai:v0.6.5
    container_name: vllm
    ports:
      - "8001:8000"
    volumes:
      - /mnt:/mnt
    command: >
      --model /mnt/models/meta-llama/meta-llama-3.1-8b-instruct
    # --gpu-memory-utilization 0.98
    # --enforce_eager
    # --max-model-len 8192